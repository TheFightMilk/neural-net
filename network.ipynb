{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Module\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation. The aim is not to create an optimized NN module with all bells and whistles, but rather a simple implementation of the method that will help me understand its inner workings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class `Network` which will be used to represent a neural network.\n",
    "\n",
    "Initializes with `sizes` matching a list of number of neurons in respective layers. E.g.: `[2,3,1]` would mean a three-layer network, with first layer containing 2 neurons, second layer with 3 neurons, and the third layer a single neuron.\n",
    "\n",
    "Biases and weights are initializes from gaussian distributions. Better starting points will be incorporated in the code later. Since the first layer (the input layer) has no bias/weight, they are not assigned for `sizes[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x)\n",
    "                       for x,y in zip(sizes[:-1],sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.04229294],\n",
       "        [1.45833354],\n",
       "        [0.17911392]]),\n",
       " array([[-1.00109272]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([2,3,1])\n",
    "net.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above arrays show the randomly generated biases. The first array are the bias from the 3 neurons in the second layer to the neuron in the final layer. The second array is the bias of the output layer.\n",
    "\n",
    "In the next step, I define the sigmoid activation function and the feedforward method to the Network class.\n",
    "\n",
    "$a' = \\sigma((w \\cdot a) + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sigmoid(z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        for b,w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w,a)+b)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I implement the stochastic gradient descent method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def SGD(self, train_dat,epochs,batch_size,eta,test_dat=None):\n",
    "        \n",
    "        train_dat = list(train_dat)\n",
    "        n = len(train_dat)\n",
    "        \n",
    "        if test_dat: \n",
    "            test_dat = list(test_dat)\n",
    "            n_test = len(test_dat)\n",
    "            \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(train_dat)\n",
    "            mini_batches = [\n",
    "                train_dat[k:k+batch_size] for k in range(0,n,batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch,eta)\n",
    "            if test_dat:\n",
    "                print(\"Epoch {} : {}/{}\".format(j,self.evaluate(test_dat),n_test));\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For each epoch, the `SGD` program shuffles the training data and creates mini batches based on the `batch_size`. Then for each mini batch a single step of gradient descent is applied (this is done by the `update_mini_batch()` code that will be written in the next chunk). It basically updates the network weights and bias. `test_dat` is an optional argument. The `SGD` program will evaluate the network on the test data after each epoch, if it has been supplied.\n",
    " \n",
    "Lets define the `update_mini_batch()` program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update_mini_batch(self,mini_batch,eta):\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
